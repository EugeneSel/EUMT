{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python376jvsc74a57bd054bfe388838f029d69968b9e925c392f1790cd451ad44dba51ac4808fa59fdcd",
   "display_name": "Python 3.7.6 64-bit ('base': conda)"
  },
  "metadata": {
   "interpreter": {
    "hash": "54bfe388838f029d69968b9e925c392f1790cd451ad44dba51ac4808fa59fdcd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## Load embedding data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_data(datafiles, tokenizer, save_filepath):\n",
    "    # data = []\n",
    "\n",
    "    for filename in datafiles:\n",
    "        with open(filename, \"r\") as f:\n",
    "            data = f.readlines()\n",
    "\n",
    "        data_preproc = [\" \".join([word.lower() for word in tokenizer._tokenize_string(line)]) for line in data]\n",
    "        print(len(data_preproc))\n",
    "\n",
    "        with open(save_filepath, \"a\") as f:\n",
    "            for line in data_preproc:\n",
    "                f.write(u\"{}\\n\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_embed_files = [\n",
    "    \"data/JW300/en.txt\",\n",
    "    # \"data/OpenSubtitles/en.txt\"\n",
    "]\n",
    "\n",
    "uk_embed_files = [\n",
    "    \"data/JW300/uk.txt\",\n",
    "    \"data/OpenSubtitles/uk.txt\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_en = []\n",
    "# data_uk = []\n",
    "\n",
    "# for filename in en_embed_files:\n",
    "#     with open(filename, \"r\") as f:\n",
    "#         data_en += f.readlines()\n",
    "\n",
    "# for filename in uk_embed_files:\n",
    "#     with open(filename, \"r\") as f:\n",
    "#         data_uk += f.readlines()"
   ]
  },
  {
   "source": [
    "## Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from opennmt.tokenizers.opennmt_tokenizer import OpenNMTTokenizer\n",
    "\n",
    "token_config = {\n",
    "    \"type\": \"OpenNMTTokenizer\",\n",
    "    \"params\": {\n",
    "        \"mode\": \"aggressive\",\n",
    "        \"joiner_annotate\": True,\n",
    "        \"segment_numbers\": True,\n",
    "        \"segment_alphabet_change\": True\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = OpenNMTTokenizer(**token_config[\"params\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# re_alpha = r\"\\w+[’'`-]?\\w{0,}\"\n",
    "\n",
    "# data_en_preproc = [\" \".join([word.lower() for word in tokenizer._tokenize_string(line)]) for line in data_en]\n",
    "# data_uk_preproc = [\" \".join([word.lower() for word in tokenizer._tokenize_string(line)]) for line in data_uk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/en_train_embeddings.txt\", \"w\") as f:\n",
    "#     for line in data_en_preproc:\n",
    "#         f.write(u\"{}\\n\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"data/uk_train_embeddings.txt\", \"w\") as f:\n",
    "#     for line in data_uk_preproc:\n",
    "#         f.write(u\"{}\\n\".format(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_data(uk_embed_files, tokenizer, \"data/uk_train_embeddings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_data(en_embed_files, tokenizer, \"data/en_train_embeddings.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/tatoeba_en-uk.tsv\", sep='\\t', header=None, index_col=0)\n",
    "df.columns = [\"en\", 'code', 'uk']\n",
    "df.drop(\"code\", axis=1, inplace=True)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                       en  \\\n",
       "0                                    Let's try something.   \n",
       "1                                  I have to go to sleep.   \n",
       "2                                      Muiriel is 20 now.   \n",
       "3                              The password is \"Muiriel\".   \n",
       "4                                    I will be back soon.   \n",
       "...                                                   ...   \n",
       "199611                                       You pervert!   \n",
       "199612                                       You pervert!   \n",
       "199613                            I'm in the post office.   \n",
       "199614  Even though John could fight, Adrion was still...   \n",
       "199615                                 This is not a hat.   \n",
       "\n",
       "                                                       uk  \n",
       "0                                 Давайте щось спробуємо!  \n",
       "1                                         Маю піти спати.  \n",
       "2                                  Мюріел зараз двадцять.  \n",
       "3                                     Пароль - \"Muiriel\".  \n",
       "4                                      Я скоро повернуся.  \n",
       "...                                                   ...  \n",
       "199611                                         Збоченець!  \n",
       "199612                                          Збоченка!  \n",
       "199613                                        Я на пошті.  \n",
       "199614  Хоча Джон і вмів битися, із нас трьох найсильн...  \n",
       "199615                                     Це не капелюх.  \n",
       "\n",
       "[199616 rows x 2 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>uk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Let's try something.</td>\n      <td>Давайте щось спробуємо!</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I have to go to sleep.</td>\n      <td>Маю піти спати.</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Muiriel is 20 now.</td>\n      <td>Мюріел зараз двадцять.</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The password is \"Muiriel\".</td>\n      <td>Пароль - \"Muiriel\".</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I will be back soon.</td>\n      <td>Я скоро повернуся.</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199611</th>\n      <td>You pervert!</td>\n      <td>Збоченець!</td>\n    </tr>\n    <tr>\n      <th>199612</th>\n      <td>You pervert!</td>\n      <td>Збоченка!</td>\n    </tr>\n    <tr>\n      <th>199613</th>\n      <td>I'm in the post office.</td>\n      <td>Я на пошті.</td>\n    </tr>\n    <tr>\n      <th>199614</th>\n      <td>Even though John could fight, Adrion was still...</td>\n      <td>Хоча Джон і вмів битися, із нас трьох найсильн...</td>\n    </tr>\n    <tr>\n      <th>199615</th>\n      <td>This is not a hat.</td>\n      <td>Це не капелюх.</td>\n    </tr>\n  </tbody>\n</table>\n<p>199616 rows × 2 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"token_len_en\"] = df.en.apply(lambda x: len(x.split()))\n",
    "df[\"token_len_uk\"] = df.uk.apply(lambda x: len(x.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        token_len_en   token_len_uk\n",
       "count  199616.000000  199616.000000\n",
       "mean        5.535623       4.789336\n",
       "std         2.329145       2.142607\n",
       "min         1.000000       1.000000\n",
       "25%         4.000000       3.000000\n",
       "50%         5.000000       4.000000\n",
       "75%         7.000000       6.000000\n",
       "max       141.000000      97.000000"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>token_len_en</th>\n      <th>token_len_uk</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>199616.000000</td>\n      <td>199616.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.535623</td>\n      <td>4.789336</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>2.329145</td>\n      <td>2.142607</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>1.000000</td>\n      <td>1.000000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>4.000000</td>\n      <td>3.000000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.000000</td>\n      <td>4.000000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>7.000000</td>\n      <td>6.000000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>141.000000</td>\n      <td>97.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"en_tokenized\"] = df.en.apply(lambda x: [word.lower() for word in word_tokenize(x)])\n",
    "df[\"uk_tokenized\"] = df.uk.apply(lambda x: [word.lower() for word in word_tokenize(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                                       en  \\\n",
       "0                                    Let's try something.   \n",
       "1                                  I have to go to sleep.   \n",
       "2                                      Muiriel is 20 now.   \n",
       "3                              The password is \"Muiriel\".   \n",
       "4                                    I will be back soon.   \n",
       "...                                                   ...   \n",
       "199611                                       You pervert!   \n",
       "199612                                       You pervert!   \n",
       "199613                            I'm in the post office.   \n",
       "199614  Even though John could fight, Adrion was still...   \n",
       "199615                                 This is not a hat.   \n",
       "\n",
       "                                                       uk  token_len_en  \\\n",
       "0                                 Давайте щось спробуємо!             3   \n",
       "1                                         Маю піти спати.             6   \n",
       "2                                  Мюріел зараз двадцять.             4   \n",
       "3                                     Пароль - \"Muiriel\".             4   \n",
       "4                                      Я скоро повернуся.             5   \n",
       "...                                                   ...           ...   \n",
       "199611                                         Збоченець!             2   \n",
       "199612                                          Збоченка!             2   \n",
       "199613                                        Я на пошті.             5   \n",
       "199614  Хоча Джон і вмів битися, із нас трьох найсильн...            16   \n",
       "199615                                     Це не капелюх.             5   \n",
       "\n",
       "        token_len_uk                                       en_tokenized  \\\n",
       "0                  3                       [let, 's, try, something, .]   \n",
       "1                  3                    [i, have, to, go, to, sleep, .]   \n",
       "2                  3                          [muiriel, is, 20, now, .]   \n",
       "3                  3            [the, password, is, ``, muiriel, '', .]   \n",
       "4                  3                       [i, will, be, back, soon, .]   \n",
       "...              ...                                                ...   \n",
       "199611             1                                  [you, pervert, !]   \n",
       "199612             1                                  [you, pervert, !]   \n",
       "199613             3                  [i, 'm, in, the, post, office, .]   \n",
       "199614            13  [even, though, john, could, fight, ,, adrion, ...   \n",
       "199615             3                         [this, is, not, a, hat, .]   \n",
       "\n",
       "                                             uk_tokenized  \n",
       "0                           [давайте, щось, спробуємо, !]  \n",
       "1                                   [маю, піти, спати, .]  \n",
       "2                            [мюріел, зараз, двадцять, .]  \n",
       "3                         [пароль, -, ``, muiriel, '', .]  \n",
       "4                                [я, скоро, повернуся, .]  \n",
       "...                                                   ...  \n",
       "199611                                     [збоченець, !]  \n",
       "199612                                      [збоченка, !]  \n",
       "199613                                  [я, на, пошті, .]  \n",
       "199614  [хоча, джон, і, вмів, битися, ,, із, нас, трьо...  \n",
       "199615                               [це, не, капелюх, .]  \n",
       "\n",
       "[199616 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>en</th>\n      <th>uk</th>\n      <th>token_len_en</th>\n      <th>token_len_uk</th>\n      <th>en_tokenized</th>\n      <th>uk_tokenized</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Let's try something.</td>\n      <td>Давайте щось спробуємо!</td>\n      <td>3</td>\n      <td>3</td>\n      <td>[let, 's, try, something, .]</td>\n      <td>[давайте, щось, спробуємо, !]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I have to go to sleep.</td>\n      <td>Маю піти спати.</td>\n      <td>6</td>\n      <td>3</td>\n      <td>[i, have, to, go, to, sleep, .]</td>\n      <td>[маю, піти, спати, .]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Muiriel is 20 now.</td>\n      <td>Мюріел зараз двадцять.</td>\n      <td>4</td>\n      <td>3</td>\n      <td>[muiriel, is, 20, now, .]</td>\n      <td>[мюріел, зараз, двадцять, .]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>The password is \"Muiriel\".</td>\n      <td>Пароль - \"Muiriel\".</td>\n      <td>4</td>\n      <td>3</td>\n      <td>[the, password, is, ``, muiriel, '', .]</td>\n      <td>[пароль, -, ``, muiriel, '', .]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>I will be back soon.</td>\n      <td>Я скоро повернуся.</td>\n      <td>5</td>\n      <td>3</td>\n      <td>[i, will, be, back, soon, .]</td>\n      <td>[я, скоро, повернуся, .]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>199611</th>\n      <td>You pervert!</td>\n      <td>Збоченець!</td>\n      <td>2</td>\n      <td>1</td>\n      <td>[you, pervert, !]</td>\n      <td>[збоченець, !]</td>\n    </tr>\n    <tr>\n      <th>199612</th>\n      <td>You pervert!</td>\n      <td>Збоченка!</td>\n      <td>2</td>\n      <td>1</td>\n      <td>[you, pervert, !]</td>\n      <td>[збоченка, !]</td>\n    </tr>\n    <tr>\n      <th>199613</th>\n      <td>I'm in the post office.</td>\n      <td>Я на пошті.</td>\n      <td>5</td>\n      <td>3</td>\n      <td>[i, 'm, in, the, post, office, .]</td>\n      <td>[я, на, пошті, .]</td>\n    </tr>\n    <tr>\n      <th>199614</th>\n      <td>Even though John could fight, Adrion was still...</td>\n      <td>Хоча Джон і вмів битися, із нас трьох найсильн...</td>\n      <td>16</td>\n      <td>13</td>\n      <td>[even, though, john, could, fight, ,, adrion, ...</td>\n      <td>[хоча, джон, і, вмів, битися, ,, із, нас, трьо...</td>\n    </tr>\n    <tr>\n      <th>199615</th>\n      <td>This is not a hat.</td>\n      <td>Це не капелюх.</td>\n      <td>5</td>\n      <td>3</td>\n      <td>[this, is, not, a, hat, .]</td>\n      <td>[це, не, капелюх, .]</td>\n    </tr>\n  </tbody>\n</table>\n<p>199616 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.en.to_csv(\"data/en_data_test.txt\")\n",
    "# df.uk.to_csv(\"data/uk_data_test.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_train_en = open(\"data/tatoeba_en_train.txt\", \"w\")\n",
    "f_train_uk = open(\"data/tatoeba_uk_train.txt\", \"w\")\n",
    "\n",
    "for idx, row in df.iterrows():\n",
    "    f_train_en.write(\" \".join(row.en_tokenized) + \"\\n\")\n",
    "    f_train_uk.write(\" \".join(row.uk_tokenized) + \"\\n\")\n",
    "\n",
    "f_train_en.close()\n",
    "f_train_uk.close()"
   ]
  },
  {
   "source": [
    "## Learn / apply fastText embeddings "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fasttext\n",
    "from models.fasttext_embeddings import train_embeddings, load_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embeddings_en = load_embeddings('models/jw300_en_embed_256.bin')\n",
    "# embeddings_uk = load_embeddings('models/jw300_uk_embed_256.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_en = train_embeddings('data/en_train_embeddings.txt', f'models/en_embed_{d}.bin', dim=d)\n",
    "embeddings_uk = train_embeddings('data/uk_train_embeddings.txt', f'models/uk_embed_{d}.bin', dim=d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.6899593472480774, 'antonello'),\n",
       " (0.6833013296127319, 'tello'),\n",
       " (0.6821622848510742, 'dello'),\n",
       " (0.6261999607086182, '￭¡￭'),\n",
       " (0.6207203269004822, 'bello'),\n",
       " (0.6008277535438538, 'quijano'),\n",
       " (0.5916381478309631, 'mommy'),\n",
       " (0.5866191387176514, 'cello'),\n",
       " (0.5746167898178101, 'huh'),\n",
       " (0.5734177827835083, 'marcello')]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    }
   ],
   "source": [
    "embeddings_en.get_nearest_neighbors(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.8191489577293396, 'привітик'),\n",
       " (0.7963200807571411, 'привітай'),\n",
       " (0.7717908620834351, 'привіти'),\n",
       " (0.7443921566009521, 'привітайся'),\n",
       " (0.7399115562438965, 'привіту'),\n",
       " (0.7350025773048401, 'привітує'),\n",
       " (0.7289767861366272, 'привітань'),\n",
       " (0.7125568985939026, 'привітом'),\n",
       " (0.668448269367218, 'привітався'),\n",
       " (0.6648420691490173, 'привітав')]"
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "embeddings_uk.get_nearest_neighbors(\"привіт\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.7763105034828186, 'franc'),\n",
       " (0.7759435176849365, 'netherlands'),\n",
       " (0.7759305834770203, 'francesca'),\n",
       " (0.7719064950942993, 'belgium'),\n",
       " (0.7451868057250977, 'spain'),\n",
       " (0.732179582118988, 'italy'),\n",
       " (0.7302939295768738, 'franck'),\n",
       " (0.7294098734855652, 'germany'),\n",
       " (0.7277642488479614, 'frances'),\n",
       " (0.7251033186912537, 'francois')]"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "embeddings_en.get_nearest_neighbors('france')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[(0.8032983541488647, 'україни'),\n",
       " (0.7992186546325684, 'українці'),\n",
       " (0.782039225101471, 'україні'),\n",
       " (0.7620049118995667, 'україною'),\n",
       " (0.7545892000198364, 'україну'),\n",
       " (0.7529142498970032, 'українська'),\n",
       " (0.751456081867218, 'країна'),\n",
       " (0.7451082468032837, 'українців'),\n",
       " (0.7433254718780518, 'українець'),\n",
       " (0.6802698373794556, 'чехія')]"
      ]
     },
     "metadata": {},
     "execution_count": 22
    }
   ],
   "source": [
    "embeddings_uk.get_nearest_neighbors('україна')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vocab_en = open(f\"data/vocab_en_{d}.txt\", \"w\")\n",
    "f_embed_en = open(f\"data/embed_en_{d}.txt\", \"w\")\n",
    "\n",
    "en_words = embeddings_en.get_words()\n",
    "en_embeds = embeddings_en.get_output_matrix()\n",
    "\n",
    "f_embed_en.write(f\"{len(en_words)} {d}\\n\")\n",
    "for word, embed_vector in zip(en_words, en_embeds):\n",
    "    f_embed_en.write(\" \".join([word] + embed_vector.astype('str').tolist()) + \"\\n\")\n",
    "    f_vocab_en.write(word + \"\\n\")\n",
    "\n",
    "f_vocab_en.close()\n",
    "f_embed_en.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_vocab_uk = open(f\"data/vocab_uk_{d}.txt\", \"w\")\n",
    "f_embed_uk = open(f\"data/embed_uk_{d}.txt\", \"w\")\n",
    "\n",
    "uk_words = embeddings_uk.get_words()\n",
    "uk_embeds = embeddings_uk.get_output_matrix()\n",
    "\n",
    "f_embed_uk.write(f\"{len(uk_words)} {d}\\n\")\n",
    "for word, embed_vector in zip(uk_words, uk_embeds):\n",
    "    f_embed_uk.write(\" \".join([word] + embed_vector.astype('str').tolist()) + \"\\n\")\n",
    "    f_vocab_uk.write(word + \"\\n\")\n",
    "\n",
    "f_vocab_uk.close()\n",
    "f_embed_uk.close()"
   ]
  },
  {
   "source": [
    "## Load translation data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_translate_files = [\n",
    "    \"data/CC/CCAligned.en-uk.en\",\n",
    "    \"data/MultiCC/MultiCCAligned.en-uk.en\",\n",
    "    \"data/WikiMatrix/WikiMatrix.en-uk.en\",\n",
    "    \"data/XLEnt/XLEnt.en-uk.en\"\n",
    "]\n",
    "\n",
    "uk_translate_files = [\n",
    "    \"data/CC/CCAligned.en-uk.uk\",\n",
    "    \"data/MultiCC/MultiCCAligned.en-uk.uk\",\n",
    "    \"data/WikiMatrix/WikiMatrix.en-uk.uk\",\n",
    "    \"data/XLEnt/XLEnt.en-uk.uk\"\n",
    "]"
   ]
  },
  {
   "source": [
    "## Tokenize"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'tokenize_data' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-8b4ba9a80a14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtokenize_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0men_translate_files\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"data/en_translate.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenize_data' is not defined"
     ]
    }
   ],
   "source": [
    "tokenize_data(en_translate_files, tokenizer, \"data/en_translate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize_data(uk_translate_files, tokenizer, \"data/uk_translate.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import opennmt\n",
    "\n",
    "config = {\n",
    "    \"model_dir\": \"data/models/fastTextTransformer/checkpoints/\",\n",
    "    \"data\": {\n",
    "        \"source_embedding\": {\n",
    "            \"path\": f\"data/embed_en_{d}.txt\",\n",
    "            \"with_header\": True,\n",
    "            \"case_insensitive\": True,\n",
    "            \"trainable\": False\n",
    "        },\n",
    "        \"target_embedding\": {\n",
    "            \"path\": f\"data/embed_uk_{d}.txt\",\n",
    "            \"with_header\": True,\n",
    "            \"case_insensitive\": True,\n",
    "            \"trainable\": False\n",
    "        },\n",
    "        \"source_vocabulary\": f\"data/vocab_en_{d}.txt\",\n",
    "        \"target_vocabulary\": f\"data/vocab_uk_{d}.txt\",\n",
    "        \"train_features_file\": \"data/tatoeba_en_train.txt\",\n",
    "        \"train_labels_file\": \"data/tatoeba_uk_train.txt\",\n",
    "    }\n",
    "}\n",
    "\n",
    "# model = opennmt.models.TransformerBase()\n",
    "# runner = opennmt.Runner(model, config, auto_config=True)\n",
    "# runner.train(num_devices=2, with_eval=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_en_embeds = opennmt.inputters.load_pretrained_embeddings(\"data/embed_en.txt\", \"data/vocab_en.txt\")\n",
    "loaded_uk_embeds = opennmt.inputters.load_pretrained_embeddings(\"data/embed_uk.txt\", \"data/vocab_uk.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'opennmt' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-235b949d0ffc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mfastTextTransformer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopennmt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTransformer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msource_embeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_embeddings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         super().__init__(\n\u001b[1;32m      4\u001b[0m             \u001b[0msource_inputter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msource_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mtarget_inputter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_embeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'opennmt' is not defined"
     ]
    }
   ],
   "source": [
    "class fastTextTransformer(opennmt.models.Transformer):\n",
    "    def __init__(self, source_embeddings, target_embeddings):\n",
    "        super().__init__(\n",
    "            source_inputter=source_embeddings,\n",
    "            target_inputter=target_embeddings,\n",
    "            num_layers=6,\n",
    "            num_units=d,\n",
    "            num_heads=8,\n",
    "            ffn_inner_dim=4 * d,\n",
    "            dropout=0.1,\n",
    "            attention_dropout=0.1,\n",
    "            ffn_dropout=0.1,\n",
    "            # share_embeddings=opennmt.models.EmbeddingsSharingLevel.ALL,\n",
    "        )\n",
    "\n",
    "model = fastTextTransformer(\n",
    "    opennmt.inputters.WordEmbedder(),\n",
    "    opennmt.inputters.WordEmbedder()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Using OpenNMT-tf version 2.17.1\n",
      "INFO:tensorflow:Using model:\n",
      "(model): fastTextTransformer(\n",
      "  (examples_inputter): SequenceToSequenceInputter(\n",
      "    (features_inputter): WordEmbedder()\n",
      "    (labels_inputter): WordEmbedder()\n",
      "    (inputters): ListWrapper(\n",
      "      (0): WordEmbedder()\n",
      "      (1): WordEmbedder()\n",
      "    )\n",
      "  )\n",
      "  (encoder): SelfAttentionEncoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionEncoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (decoder): SelfAttentionDecoder(\n",
      "    (position_encoder): SinusoidalPositionEncoder(\n",
      "      (reducer): SumReducer()\n",
      "    )\n",
      "    (layer_norm): LayerNorm()\n",
      "    (layers): ListWrapper(\n",
      "      (0): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(256)\n",
      "              (linear_keys): Dense(256)\n",
      "              (linear_values): Dense(256)\n",
      "              (linear_output): Dense(256)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (1): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(256)\n",
      "              (linear_keys): Dense(256)\n",
      "              (linear_values): Dense(256)\n",
      "              (linear_output): Dense(256)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (2): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(256)\n",
      "              (linear_keys): Dense(256)\n",
      "              (linear_values): Dense(256)\n",
      "              (linear_output): Dense(256)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (3): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(256)\n",
      "              (linear_keys): Dense(256)\n",
      "              (linear_values): Dense(256)\n",
      "              (linear_output): Dense(256)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (4): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(256)\n",
      "              (linear_keys): Dense(256)\n",
      "              (linear_values): Dense(256)\n",
      "              (linear_output): Dense(256)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "      (5): SelfAttentionDecoderLayer(\n",
      "        (self_attention): TransformerLayerWrapper(\n",
      "          (layer): MultiHeadAttention(\n",
      "            (linear_queries): Dense(256)\n",
      "            (linear_keys): Dense(256)\n",
      "            (linear_values): Dense(256)\n",
      "            (linear_output): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "        (attention): ListWrapper(\n",
      "          (0): TransformerLayerWrapper(\n",
      "            (layer): MultiHeadAttention(\n",
      "              (linear_queries): Dense(256)\n",
      "              (linear_keys): Dense(256)\n",
      "              (linear_values): Dense(256)\n",
      "              (linear_output): Dense(256)\n",
      "            )\n",
      "            (input_layer_norm): LayerNorm()\n",
      "          )\n",
      "        )\n",
      "        (ffn): TransformerLayerWrapper(\n",
      "          (layer): FeedForwardNetwork(\n",
      "            (inner): Dense(1024)\n",
      "            (outer): Dense(256)\n",
      "          )\n",
      "          (input_layer_norm): LayerNorm()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "WARNING:tensorflow:No GPU is detected, falling back to CPU\n",
      "INFO:tensorflow:Using parameters:\n",
      "data:\n",
      "  source_embedding:\n",
      "    case_insensitive: true\n",
      "    path: data/embed_en_256.txt\n",
      "    trainable: false\n",
      "    with_header: true\n",
      "  source_vocabulary: data/vocab_en_256.txt\n",
      "  target_embedding:\n",
      "    case_insensitive: true\n",
      "    path: data/embed_uk_256.txt\n",
      "    trainable: false\n",
      "    with_header: true\n",
      "  target_vocabulary: data/vocab_uk_256.txt\n",
      "  train_features_file: data/tatoeba_en_train.txt\n",
      "  train_labels_file: data/tatoeba_uk_train.txt\n",
      "eval:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "infer:\n",
      "  batch_size: 32\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "model_dir: data/models/fastTextTransformer/checkpoints/\n",
      "params:\n",
      "  average_loss_in_time: true\n",
      "  beam_width: 4\n",
      "  decay_params:\n",
      "    model_dim: 256\n",
      "    warmup_steps: 8000\n",
      "  decay_type: NoamDecay\n",
      "  label_smoothing: 0.1\n",
      "  learning_rate: 2.0\n",
      "  num_hypotheses: 1\n",
      "  optimizer: LazyAdam\n",
      "  optimizer_params:\n",
      "    beta_1: 0.9\n",
      "    beta_2: 0.998\n",
      "score:\n",
      "  batch_size: 64\n",
      "  batch_type: examples\n",
      "  length_bucket_width: 5\n",
      "train:\n",
      "  average_last_checkpoints: 8\n",
      "  batch_size: 3072\n",
      "  batch_type: tokens\n",
      "  effective_batch_size: 25000\n",
      "  keep_checkpoint_max: 8\n",
      "  length_bucket_width: 1\n",
      "  max_step: 500000\n",
      "  maximum_features_length: 100\n",
      "  maximum_labels_length: 100\n",
      "  sample_buffer_size: -1\n",
      "  save_summary_steps: 100\n",
      "\n",
      "WARNING:tensorflow:No checkpoint to restore in data/models/fastTextTransformer/checkpoints/\n",
      "INFO:tensorflow:Accumulate gradients of 9 iterations to reach effective batch size of 25000\n",
      "INFO:tensorflow:Training on 199616 examples\n",
      "INFO:tensorflow:Number of model parameters: 75918869\n",
      "WARNING:tensorflow:AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdba6394e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <bound method Socket.send of <zmq.sugar.socket.Socket object at 0x7fdba6394e50>> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module, class, method, function, traceback, frame, or code object was expected, got cython_function_or_method\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "INFO:tensorflow:Number of model weights: 260 (trainable = 258, non trainable = 2)\n"
     ]
    }
   ],
   "source": [
    "runner = opennmt.Runner(model, config, auto_config=True)\n",
    "runner.train(num_devices=1, with_eval=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f8a688cae5d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}